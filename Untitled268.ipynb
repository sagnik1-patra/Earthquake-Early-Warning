{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3f361f-b433-4e46-9424-b2f2baead1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_36096\\3063080609.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_36096\\3063080609.py:39: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_36096\\3063080609.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_36096\\3063080609.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_36096\\3063080609.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_36096\\3063080609.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_36096\\3063080609.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_36096\\3063080609.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_36096\\3063080609.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running AIS + PSO Hybrid Optimizer...\n",
      "[AIS+PSO] Generation 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "[AIS+PSO] Generation 2/5\n",
      "[AIS+PSO] Generation 3/5\n",
      "[AIS+PSO] Generation 4/5\n",
      "[AIS+PSO] Generation 5/5\n",
      "\n",
      "[INFO] BEST HYPERPARAMETERS: {'filters': 64, 'kernel_size': 3, 'lstm_units': 64, 'dropout': 0.1, 'lr': 0.001}\n",
      "469/469 [==============================] - 194s 9ms/step - loss: 22103710951599505408.0000 - mae: 2396482816.0000 - val_loss: 21687743712579289088.0000 - val_mae: 2351397888.0000\n",
      "Epoch 2/25\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 22103581209227427840.0000 - mae: 2396511488.0000 - val_loss: 21687537004393267200.0000 - val_mae: 2351425792.0000\n",
      "Epoch 3/25\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 22103244758669328384.0000 - mae: 2396549888.0000 - val_loss: 21687035627091001344.0000 - val_mae: 2351494656.0000\n",
      "Epoch 4/25\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 22102525678064762880.0000 - mae: 2396642304.0000 - val_loss: 21686153818765524992.0000 - val_mae: 2351615744.0000\n",
      "Epoch 5/25\n",
      "469/469 [==============================] - 4s 7ms/step - loss: 22101421768390475776.0000 - mae: 2396789504.0000 - val_loss: 21684858594068004864.0000 - val_mae: 2351793920.0000\n",
      "Epoch 6/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22099853864809267200.0000 - mae: 2396992512.0000 - val_loss: 21683119166672863232.0000 - val_mae: 2352034304.0000\n",
      "Epoch 7/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22097852753646714880.0000 - mae: 2397257472.0000 - val_loss: 21680900352208011264.0000 - val_mae: 2352339712.0000\n",
      "Epoch 8/25\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 22095317279833063424.0000 - mae: 2397596928.0000 - val_loss: 21678191155557171200.0000 - val_mae: 2352713728.0000\n",
      "Epoch 9/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22092295821879934976.0000 - mae: 2397993216.0000 - val_loss: 21674951994301743104.0000 - val_mae: 2353160704.0000\n",
      "Epoch 10/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22088696020810596352.0000 - mae: 2398460416.0000 - val_loss: 21671158679185915904.0000 - val_mae: 2353685504.0000\n",
      "Epoch 11/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22084531070764580864.0000 - mae: 2399027968.0000 - val_loss: 21666809011186434048.0000 - val_mae: 2354287104.0000\n",
      "Epoch 12/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22079763588346544128.0000 - mae: 2399646720.0000 - val_loss: 21661876602024230912.0000 - val_mae: 2354970112.0000\n",
      "Epoch 13/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22074417762812297216.0000 - mae: 2400353536.0000 - val_loss: 21656306476117917696.0000 - val_mae: 2355742720.0000\n",
      "Epoch 14/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22068372647882784768.0000 - mae: 2401161472.0000 - val_loss: 21650125021746561024.0000 - val_mae: 2356600320.0000\n",
      "Epoch 15/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22061630442581262336.0000 - mae: 2402053120.0000 - val_loss: 21643261870165983232.0000 - val_mae: 2357552640.0000\n",
      "Epoch 16/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22054402253140262912.0000 - mae: 2403022848.0000 - val_loss: 21635741210631995392.0000 - val_mae: 2358597120.0000\n",
      "Epoch 17/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22046411002629586944.0000 - mae: 2404096768.0000 - val_loss: 21627571839237619712.0000 - val_mae: 2359734528.0000\n",
      "Epoch 18/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22037612710584123392.0000 - mae: 2405273088.0000 - val_loss: 21618646003843334144.0000 - val_mae: 2360978944.0000\n",
      "Epoch 19/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22027967794585272320.0000 - mae: 2406548992.0000 - val_loss: 21608921923007283200.0000 - val_mae: 2362334976.0000\n",
      "Epoch 20/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22017632385284177920.0000 - mae: 2407912960.0000 - val_loss: 21598483159613177856.0000 - val_mae: 2363793664.0000\n",
      "Epoch 21/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22006588890494795776.0000 - mae: 2409393408.0000 - val_loss: 21587213165428473856.0000 - val_mae: 2365369856.0000\n",
      "Epoch 22/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21994641597147381760.0000 - mae: 2410991360.0000 - val_loss: 21575142726778748928.0000 - val_mae: 2367061504.0000\n",
      "Epoch 23/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21982076378265157632.0000 - mae: 2412736768.0000 - val_loss: 21562282838780280832.0000 - val_mae: 2368867328.0000\n",
      "Epoch 24/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21968407249708646400.0000 - mae: 2414511360.0000 - val_loss: 21548516953200525312.0000 - val_mae: 2370802688.0000\n",
      "Epoch 25/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21953695784129003520.0000 - mae: 2416493312.0000 - val_loss: 21533836273946460160.0000 - val_mae: 2372870400.0000\n",
      "147/147 [==============================] - 1s 3ms/step\n",
      "[RESULT] MAE=2408178688.0000 | RMSE=4675498911.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] All hybrid_ files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Dropout, Flatten, LSTM, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ==============================================================\n",
    "# PATHS\n",
    "# ==============================================================\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\NXTWAVE\\Downloads\\Earthquake Early Warning\\archive\\database.csv\"\n",
    "BASE_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Earthquake Early Warning\"\n",
    "VISUAL_DIR = os.path.join(BASE_DIR, \"visuals\")\n",
    "\n",
    "os.makedirs(VISUAL_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================\n",
    "# LOAD DATA + CLEANING\n",
    "# ==============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Convert date columns\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[col] = df[col].astype(np.int64) // 10**9\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Keep numeric only\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Remove missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Features + target\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "# ==============================================================\n",
    "# SCALING + RESHAPE\n",
    "# ==============================================================\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==============================================================\n",
    "# CNN + LSTM MODEL\n",
    "# ==============================================================\n",
    "\n",
    "def build_model(hp):\n",
    "    inp = Input(shape=(X_train.shape[1], 1))\n",
    "\n",
    "    x = Conv1D(filters=hp[\"filters\"], kernel_size=hp[\"kernel_size\"], activation=\"relu\")(inp)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    y = LSTM(hp[\"lstm_units\"], return_sequences=False)(inp)\n",
    "\n",
    "    c = Concatenate()([x, y])\n",
    "    c = Dense(64, activation=\"relu\")(c)\n",
    "    c = Dropout(hp[\"dropout\"])(c)\n",
    "    out = Dense(1)(c)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=Adam(hp[\"lr\"]), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# ==============================================================\n",
    "# HYBRID AIS + PSO OPTIMIZER\n",
    "# ==============================================================\n",
    "\n",
    "def hybrid_ais_pso_optimize():\n",
    "\n",
    "    # Hyperparameter search space\n",
    "    search_params = {\n",
    "        \"filters\": [16, 32, 64],\n",
    "        \"kernel_size\": [2, 3, 5],\n",
    "        \"lstm_units\": [16, 32, 64],\n",
    "        \"dropout\": [0.1, 0.2, 0.3],\n",
    "        \"lr\": [0.001, 0.0005]\n",
    "    }\n",
    "\n",
    "    def random_hp():\n",
    "        return {\n",
    "            \"filters\": random.choice(search_params[\"filters\"]),\n",
    "            \"kernel_size\": random.choice(search_params[\"kernel_size\"]),\n",
    "            \"lstm_units\": random.choice(search_params[\"lstm_units\"]),\n",
    "            \"dropout\": random.choice(search_params[\"dropout\"]),\n",
    "            \"lr\": random.choice(search_params[\"lr\"])\n",
    "        }\n",
    "\n",
    "    # Fitness function (validation loss)\n",
    "    def fitness(hp):\n",
    "        model = build_model(hp)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=4,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=0\n",
    "        )\n",
    "        return history.history[\"val_loss\"][-1]\n",
    "\n",
    "    # ---- AIS (CLONALG) + PSO ----\n",
    "    population = [random_hp() for _ in range(8)]\n",
    "    velocities = [{} for _ in population]\n",
    "    best_particle = None\n",
    "    best_fitness = float(\"inf\")\n",
    "\n",
    "    for generation in range(5):\n",
    "        print(f\"[AIS+PSO] Generation {generation+1}/5\")\n",
    "\n",
    "        scores = []\n",
    "        for i, hp in enumerate(population):\n",
    "            score = fitness(hp)\n",
    "            scores.append(score)\n",
    "\n",
    "            if score < best_fitness:\n",
    "                best_fitness = score\n",
    "                best_particle = hp\n",
    "\n",
    "        # Clone top 50%\n",
    "        sorted_idx = np.argsort(scores)\n",
    "        top_half = [population[i] for i in sorted_idx[:4]]\n",
    "        \n",
    "        clones = []\n",
    "        for parent in top_half:\n",
    "            for _ in range(2):\n",
    "                mutated = parent.copy()\n",
    "                key = random.choice(list(mutated.keys()))\n",
    "                mutated[key] = random.choice(search_params[key])\n",
    "                clones.append(mutated)\n",
    "\n",
    "        # PSO velocity update\n",
    "        new_population = []\n",
    "        for i in range(8):\n",
    "            new_hp = population[i].copy()\n",
    "            for key in new_hp:\n",
    "                if random.random() < 0.3:\n",
    "                    new_hp[key] = best_particle[key]\n",
    "            new_population.append(new_hp)\n",
    "\n",
    "        population = new_population + clones\n",
    "        population = population[:8]\n",
    "\n",
    "    return best_particle\n",
    "\n",
    "# ==============================================================\n",
    "# RUN HYBRID OPTIMIZER\n",
    "# ==============================================================\n",
    "\n",
    "print(\"[INFO] Running AIS + PSO Hybrid Optimizer...\")\n",
    "best_hp = hybrid_ais_pso_optimize()\n",
    "print(\"\\n[INFO] BEST HYPERPARAMETERS:\", best_hp)\n",
    "\n",
    "# ==============================================================\n",
    "# TRAIN FINAL MODEL\n",
    "# ==============================================================\n",
    "\n",
    "final_model = build_model(best_hp)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==============================================================\n",
    "# EVALUATION\n",
    "# ==============================================================\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"[RESULT] MAE={mae:.4f} | RMSE={rmse:.4f}\")\n",
    "\n",
    "# ==============================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================\n",
    "\n",
    "# Model\n",
    "final_model.save(os.path.join(BASE_DIR, \"hybrid_quakeguard_model.h5\"))\n",
    "\n",
    "# Scaler\n",
    "pickle.dump(scaler, open(os.path.join(BASE_DIR, \"hybrid_quakeguard_scaler.pkl\"), \"wb\"))\n",
    "\n",
    "# Config\n",
    "with open(os.path.join(BASE_DIR, \"hybrid_quakeguard_config.yaml\"), \"w\") as f:\n",
    "    yaml.dump({\"best_hyperparameters\": best_hp}, f)\n",
    "\n",
    "# Predictions JSON\n",
    "with open(os.path.join(BASE_DIR, \"hybrid_quakeguard_predictions.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"y_test\": y_test.flatten().tolist(),\n",
    "        \"predictions\": y_pred.flatten().tolist(),\n",
    "        \"mae\": float(mae),\n",
    "        \"rmse\": float(rmse)\n",
    "    }, f)\n",
    "\n",
    "# ==============================================================\n",
    "# VISUAL GRAPHS\n",
    "# ==============================================================\n",
    "\n",
    "# ðŸ”¥ 1. HEATMAP\n",
    "plt.figure(figsize=(6,5))\n",
    "corr = pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred.flatten()}).corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Hybrid QuakeGuard Heatmap\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"hybrid_quakeguard_heatmap.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”¥ 2. ACCURACY (MAE)\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"mae\"], label=\"Training MAE\")\n",
    "plt.plot(history.history[\"val_mae\"], label=\"Validation MAE\")\n",
    "plt.legend()\n",
    "plt.title(\"Hybrid QuakeGuard Accuracy\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"hybrid_quakeguard_accuracy.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”¥ 3. LOSS\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Hybrid QuakeGuard Loss\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"hybrid_quakeguard_loss.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”¥ 4. COMPARISON (Line Graph)\n",
    "plt.figure()\n",
    "plt.plot(y_test[:200], label=\"Actual\")\n",
    "plt.plot(y_pred[:200], label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.title(\"Hybrid QuakeGuard Comparison Graph\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"hybrid_quakeguard_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”¥ 5. PREDICTION SCATTER\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Hybrid QuakeGuard Prediction Scatter\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"hybrid_quakeguard_prediction.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ðŸ”¥ 6. RESULT DISTRIBUTION\n",
    "plt.figure()\n",
    "plt.hist(y_pred, bins=30, alpha=0.7)\n",
    "plt.title(\"Hybrid QuakeGuard Result Distribution\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"hybrid_quakeguard_result_distribution.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n[INFO] All hybrid_ files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6c84b-61f4-4695-8abc-b5bb88acee21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
