{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ff1eff-c7f1-4722-a7ea-e7ab8dd84340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_6200\\1468188578.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_6200\\1468188578.py:38: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_6200\\1468188578.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_6200\\1468188578.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_6200\\1468188578.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_6200\\1468188578.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_6200\\1468188578.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_6200\\1468188578.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_6200\\1468188578.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running Hybrid PSO + QPSO...\n",
      "[HYBRID] Iteration 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "[HYBRID] Iteration 2/5\n",
      "[HYBRID] Iteration 3/5\n",
      "[HYBRID] Iteration 4/5\n",
      "[HYBRID] Iteration 5/5\n",
      "[INFO] BEST HYPERPARAMETERS: {'filters': 64, 'kernel_size': 5, 'lstm_units': 64, 'dropout': 0.1, 'lr': 0.001}\n",
      "Epoch 1/25\n",
      "469/469 [==============================] - 7s 8ms/step - loss: 22103704354529738752.0000 - mae: 2396482816.0000 - val_loss: 21687754707695566848.0000 - val_mae: 2351396864.0000\n",
      "Epoch 2/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22103581209227427840.0000 - mae: 2396510208.0000 - val_loss: 21687567790718844928.0000 - val_mae: 2351420160.0000\n",
      "Epoch 3/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22103297535227461632.0000 - mae: 2396538624.0000 - val_loss: 21687127986067734528.0000 - val_mae: 2351482368.0000\n",
      "Epoch 4/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22102662017506607104.0000 - mae: 2396623360.0000 - val_loss: 21686364924998057984.0000 - val_mae: 2351587328.0000\n",
      "Epoch 5/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22101707641413697536.0000 - mae: 2396749312.0000 - val_loss: 21685245622160982016.0000 - val_mae: 2351742464.0000\n",
      "Epoch 6/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22100388227460366336.0000 - mae: 2396928768.0000 - val_loss: 21683737092207673344.0000 - val_mae: 2351951616.0000\n",
      "Epoch 7/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22098624610809413632.0000 - mae: 2397161984.0000 - val_loss: 21681832738068365312.0000 - val_mae: 2352215808.0000\n",
      "Epoch 8/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22096436582670139392.0000 - mae: 2397446912.0000 - val_loss: 21679475385138413568.0000 - val_mae: 2352542208.0000\n",
      "Epoch 9/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22093795555740221440.0000 - mae: 2397801984.0000 - val_loss: 21676676028534095872.0000 - val_mae: 2352930304.0000\n",
      "Epoch 10/25\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 22090712525135937536.0000 - mae: 2398205696.0000 - val_loss: 21673399483883323392.0000 - val_mae: 2353385216.0000\n",
      "Epoch 11/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22087097330903810048.0000 - mae: 2398688768.0000 - val_loss: 21669641353139585024.0000 - val_mae: 2353908480.0000\n",
      "Epoch 12/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22082960968160116736.0000 - mae: 2399235584.0000 - val_loss: 21665357655837769728.0000 - val_mae: 2354503168.0000\n",
      "Epoch 13/25\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 22078334223230435328.0000 - mae: 2399868928.0000 - val_loss: 21660574780256944128.0000 - val_mae: 2355168768.0000\n",
      "Epoch 14/25\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 22073170916626399232.0000 - mae: 2400557312.0000 - val_loss: 21655231153745952768.0000 - val_mae: 2355913472.0000\n",
      "Epoch 15/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22067378689371275264.0000 - mae: 2401330176.0000 - val_loss: 21649315781188517888.0000 - val_mae: 2356735488.0000\n",
      "Epoch 16/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22061008118999941120.0000 - mae: 2402170880.0000 - val_loss: 21642782483096272896.0000 - val_mae: 2357646592.0000\n",
      "Epoch 17/25\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 22054092190861230080.0000 - mae: 2403115008.0000 - val_loss: 21635679637980839936.0000 - val_mae: 2358636032.0000\n",
      "Epoch 18/25\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 22046320842676109312.0000 - mae: 2404103936.0000 - val_loss: 21627884100539908096.0000 - val_mae: 2359722752.0000\n",
      "Epoch 19/25\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 22038144874211966976.0000 - mae: 2405250560.0000 - val_loss: 21619433254168821760.0000 - val_mae: 2360903680.0000\n",
      "Epoch 20/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22029010131608403968.0000 - mae: 2406415104.0000 - val_loss: 21610296312542003200.0000 - val_mae: 2362177536.0000\n",
      "Epoch 21/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22019512550167674880.0000 - mae: 2407752960.0000 - val_loss: 21600519455147819008.0000 - val_mae: 2363540992.0000\n",
      "Epoch 22/25\n",
      "469/469 [==============================] - 4s 7ms/step - loss: 22008961636587536384.0000 - mae: 2409096192.0000 - val_loss: 21589997128870002688.0000 - val_mae: 2365013248.0000\n",
      "Epoch 23/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21997704836542365696.0000 - mae: 2410596096.0000 - val_loss: 21578711741522509824.0000 - val_mae: 2366590720.0000\n",
      "Epoch 24/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21985816916822851584.0000 - mae: 2412202752.0000 - val_loss: 21566751254035562496.0000 - val_mae: 2368264960.0000\n",
      "Epoch 25/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21973368246173171712.0000 - mae: 2413924096.0000 - val_loss: 21554047496688238592.0000 - val_mae: 2370043136.0000\n",
      "147/147 [==============================] - 1s 3ms/step\n",
      "\n",
      "[RESULT] MAE=2405442560.0000 | RMSE=4677685195.6475\n",
      "[INFO] Files saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved all graphs!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pickle\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Dropout, Flatten, LSTM, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ==============================================================\n",
    "# PATHS\n",
    "# ==============================================================\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\NXTWAVE\\Downloads\\Earthquake Early Warning\\archive\\database.csv\"\n",
    "BASE_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Earthquake Early Warning\"\n",
    "VISUAL_DIR = os.path.join(BASE_DIR, \"visuals\")\n",
    "\n",
    "os.makedirs(VISUAL_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================\n",
    "# LOAD DATA\n",
    "# ==============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# FIX 1: Convert date columns to numeric timestamps\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[col] = df[col].astype(np.int64) // 10**9\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# FIX 2: Remove non-numeric columns completely\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# FIX 3: Drop rows with missing values\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Make sure dataset still valid\n",
    "if df.shape[1] < 3:\n",
    "    raise ValueError(\"Dataset has too few numeric features after cleaning!\")\n",
    "\n",
    "# Last column = target\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "# ==============================================================\n",
    "# SCALING & RESHAPING\n",
    "# ==============================================================\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# reshape for CNN + LSTM\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==============================================================\n",
    "# HYBRID PSO + QPSO OPTIMIZER\n",
    "# ==============================================================\n",
    "\n",
    "import random\n",
    "\n",
    "def hybrid_pso_qpso_optimize():\n",
    "    swarm_size = 6\n",
    "    iterations = 5\n",
    "\n",
    "    search_space = {\n",
    "        \"filters\": [16, 32, 64],\n",
    "        \"kernel_size\": [2, 3, 5],\n",
    "        \"lstm_units\": [16, 32, 64],\n",
    "        \"dropout\": [0.1, 0.2],\n",
    "        \"lr\": [0.001, 0.0005]\n",
    "    }\n",
    "\n",
    "    def random_hp():\n",
    "        return {\n",
    "            \"filters\": random.choice(search_space[\"filters\"]),\n",
    "            \"kernel_size\": random.choice(search_space[\"kernel_size\"]),\n",
    "            \"lstm_units\": random.choice(search_space[\"lstm_units\"]),\n",
    "            \"dropout\": random.choice(search_space[\"dropout\"]),\n",
    "            \"lr\": random.choice(search_space[\"lr\"])\n",
    "        }\n",
    "\n",
    "    def fitness(hp):\n",
    "        model = build_model(hp)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=4,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=0\n",
    "        )\n",
    "        return history.history[\"val_loss\"][-1]\n",
    "\n",
    "    swarm = [random_hp() for _ in range(swarm_size)]\n",
    "    best_hp = None\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for it in range(iterations):\n",
    "        print(f\"[HYBRID] Iteration {it+1}/{iterations}\")\n",
    "\n",
    "        for particle in swarm:\n",
    "            loss = fitness(particle)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_hp = particle\n",
    "\n",
    "        # QPSO update\n",
    "        for particle in swarm:\n",
    "            for key in particle:\n",
    "                if random.random() < 0.5:\n",
    "                    particle[key] = best_hp[key]\n",
    "\n",
    "    return best_hp\n",
    "\n",
    "# ==============================================================\n",
    "# BUILD MODEL\n",
    "# ==============================================================\n",
    "\n",
    "def build_model(hp):\n",
    "    inp = Input(shape=(X_train.shape[1], 1))\n",
    "\n",
    "    x = Conv1D(filters=hp[\"filters\"], kernel_size=hp[\"kernel_size\"], activation=\"relu\")(inp)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    y = LSTM(hp[\"lstm_units\"], return_sequences=False)(inp)\n",
    "\n",
    "    combined = Concatenate()([x, y])\n",
    "    combined = Dense(64, activation=\"relu\")(combined)\n",
    "    combined = Dropout(hp[\"dropout\"])(combined)\n",
    "    out = Dense(1)(combined)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=Adam(hp[\"lr\"]), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# ==============================================================\n",
    "# RUN OPTIMIZER\n",
    "# ==============================================================\n",
    "\n",
    "print(\"[INFO] Running Hybrid PSO + QPSO...\")\n",
    "best_hp = hybrid_pso_qpso_optimize()\n",
    "print(\"[INFO] BEST HYPERPARAMETERS:\", best_hp)\n",
    "\n",
    "# ==============================================================\n",
    "# TRAIN FINAL MODEL\n",
    "# ==============================================================\n",
    "\n",
    "final_model = build_model(best_hp)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    callbacks=[early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==============================================================\n",
    "# EVALUATE\n",
    "# ==============================================================\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\n[RESULT] MAE={mae:.4f} | RMSE={rmse:.4f}\")\n",
    "\n",
    "# ==============================================================\n",
    "# SAVE FILES\n",
    "# ==============================================================\n",
    "\n",
    "# model\n",
    "final_model.save(os.path.join(BASE_DIR, \"pso_qpso_quakeguard_model.h5\"))\n",
    "\n",
    "# scaler\n",
    "pickle.dump(scaler, open(os.path.join(BASE_DIR, \"pso_qpso_quakeguard_scaler.pkl\"), \"wb\"))\n",
    "\n",
    "# yaml config\n",
    "with open(os.path.join(BASE_DIR, \"pso_qpso_quakeguard_config.yaml\"), \"w\") as f:\n",
    "    yaml.dump({\"best_hyperparameters\": best_hp}, f)\n",
    "\n",
    "# json predictions\n",
    "with open(os.path.join(BASE_DIR, \"quakeguard_predictions.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"y_test\": y_test.flatten().tolist(),\n",
    "        \"pred\": y_pred.flatten().tolist()\n",
    "    }, f)\n",
    "\n",
    "print(\"[INFO] Files saved successfully.\")\n",
    "\n",
    "# ==============================================================\n",
    "# VISUALS\n",
    "# ==============================================================\n",
    "\n",
    "# accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"mae\"], label=\"train mae\")\n",
    "plt.plot(history.history[\"val_mae\"], label=\"val mae\")\n",
    "plt.legend()\n",
    "plt.title(\"QuakeGuard Accuracy\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"quakeguard_accuracy_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# loss\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.title(\"QuakeGuard Loss\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"quakeguard_loss_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"[INFO] Saved all graphs!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a0b85-0f37-4e75-ab5c-7c4105127122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
