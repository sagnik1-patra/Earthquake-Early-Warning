{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f452f52-945b-41c7-aa1e-c826a749f38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_39248\\120319551.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_39248\\120319551.py:38: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_39248\\120319551.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_39248\\120319551.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_39248\\120319551.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_39248\\120319551.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_39248\\120319551.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_39248\\120319551.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_39248\\120319551.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARALLEL HYBRID] Generation 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "[PARALLEL HYBRID] Generation 2/5\n",
      "[PARALLEL HYBRID] Generation 3/5\n",
      "[PARALLEL HYBRID] Generation 4/5\n",
      "[PARALLEL HYBRID] Generation 5/5\n",
      "\n",
      "[INFO] BEST HYPERPARAMETERS: {'filters': 64, 'kernel_size': 2, 'lstm_units': 64, 'dropout': 0.2, 'lr': 0.001}\n",
      "Epoch 1/25\n",
      "469/469 [==============================] - 7s 9ms/step - loss: 22103706553552994304.0000 - mae: 2396482816.0000 - val_loss: 21687739314532777984.0000 - val_mae: 2351398656.0000\n",
      "Epoch 2/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22103574612157661184.0000 - mae: 2396511488.0000 - val_loss: 21687490824904900608.0000 - val_mae: 2351432960.0000\n",
      "Epoch 3/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22103167792855384064.0000 - mae: 2396563200.0000 - val_loss: 21686899287649157120.0000 - val_mae: 2351512576.0000\n",
      "Epoch 4/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22102338761088040960.0000 - mae: 2396668928.0000 - val_loss: 21685878940858580992.0000 - val_mae: 2351652608.0000\n",
      "Epoch 5/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22101054531506798592.0000 - mae: 2396835584.0000 - val_loss: 21684379206998294528.0000 - val_mae: 2351858944.0000\n",
      "Epoch 6/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22099249133413990400.0000 - mae: 2397073152.0000 - val_loss: 21682342911463653376.0000 - val_mae: 2352137472.0000\n",
      "Epoch 7/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22096900576577060864.0000 - mae: 2397381120.0000 - val_loss: 21679781049370935296.0000 - val_mae: 2352490240.0000\n",
      "Epoch 8/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22093991268809965568.0000 - mae: 2397769984.0000 - val_loss: 21676634247092240384.0000 - val_mae: 2352922624.0000\n",
      "Epoch 9/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22090488224763871232.0000 - mae: 2398229760.0000 - val_loss: 21672876116348502016.0000 - val_mae: 2353439232.0000\n",
      "Epoch 10/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22086314478624833536.0000 - mae: 2398772736.0000 - val_loss: 21668497861046697984.0000 - val_mae: 2354044416.0000\n",
      "Epoch 11/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22081538200113774592.0000 - mae: 2399414784.0000 - val_loss: 21663468694861250560.0000 - val_mae: 2354735360.0000\n",
      "Epoch 12/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22076078025370238976.0000 - mae: 2400136192.0000 - val_loss: 21657762229513093120.0000 - val_mae: 2355521280.0000\n",
      "Epoch 13/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22069896570998882304.0000 - mae: 2400962816.0000 - val_loss: 21651321290397581312.0000 - val_mae: 2356410368.0000\n",
      "Epoch 14/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22062949856534593536.0000 - mae: 2401877760.0000 - val_loss: 21644139280444948480.0000 - val_mae: 2357401600.0000\n",
      "Epoch 15/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22055244479047139328.0000 - mae: 2402888960.0000 - val_loss: 21636222796724961280.0000 - val_mae: 2358496000.0000\n",
      "Epoch 16/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22046721064908619776.0000 - mae: 2404023040.0000 - val_loss: 21627519062679486464.0000 - val_mae: 2359699712.0000\n",
      "Epoch 17/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22037491764305068032.0000 - mae: 2405239552.0000 - val_loss: 21617981898820157440.0000 - val_mae: 2361022208.0000\n",
      "Epoch 18/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22027475213376028672.0000 - mae: 2406607616.0000 - val_loss: 21607646489519063040.0000 - val_mae: 2362456064.0000\n",
      "Epoch 19/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22016389937144791040.0000 - mae: 2408051712.0000 - val_loss: 21596449063101792256.0000 - val_mae: 2364012288.0000\n",
      "Epoch 20/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22004392066262499328.0000 - mae: 2409641728.0000 - val_loss: 21584354435196256256.0000 - val_mae: 2365694976.0000\n",
      "Epoch 21/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21991690507938430976.0000 - mae: 2411337984.0000 - val_loss: 21571369202872221696.0000 - val_mae: 2367502336.0000\n",
      "Epoch 22/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21977737705381953536.0000 - mae: 2413136128.0000 - val_loss: 21557422997385510912.0000 - val_mae: 2369450240.0000\n",
      "Epoch 23/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21963290122592976896.0000 - mae: 2415156736.0000 - val_loss: 21542515818736123904.0000 - val_mae: 2371535616.0000\n",
      "Epoch 24/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21947666062362279936.0000 - mae: 2417219328.0000 - val_loss: 21526616880598482944.0000 - val_mae: 2373764096.0000\n",
      "Epoch 25/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 21931030451434029056.0000 - mae: 2419510784.0000 - val_loss: 21509820740972576768.0000 - val_mae: 2376122368.0000\n",
      "147/147 [==============================] - 1s 3ms/step\n",
      "[RESULT] MAE=2411324928.0000 | RMSE=4672937267.8299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] All parallel_ files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Dropout, Flatten, LSTM, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ==============================================================\n",
    "# PATHS\n",
    "# ==============================================================\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\NXTWAVE\\Downloads\\Earthquake Early Warning\\archive\\database.csv\"\n",
    "BASE_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Earthquake Early Warning\"\n",
    "VISUAL_DIR = os.path.join(BASE_DIR, \"visuals\")\n",
    "os.makedirs(VISUAL_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================\n",
    "# LOAD + CLEAN DATA\n",
    "# ==============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Convert date columns to numeric timestamps\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[col] = df[col].astype(np.int64) // 10**9\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Only numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "df = df.dropna()\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "# ==============================================================\n",
    "# SCALING + RESHAPE\n",
    "# ==============================================================\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==============================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ==============================================================\n",
    "\n",
    "def build_model(hp):\n",
    "    inp = Input(shape=(X_train.shape[1], 1))\n",
    "\n",
    "    x = Conv1D(hp[\"filters\"], hp[\"kernel_size\"], activation=\"relu\")(inp)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    y = LSTM(hp[\"lstm_units\"], return_sequences=False)(inp)\n",
    "\n",
    "    c = Concatenate()([x, y])\n",
    "    c = Dense(64, activation=\"relu\")(c)\n",
    "    c = Dropout(hp[\"dropout\"])(c)\n",
    "    out = Dense(1)(c)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=Adam(hp[\"lr\"]), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# ==============================================================\n",
    "# PARALLEL HYBRID AIS + PSO\n",
    "# ==============================================================\n",
    "\n",
    "def parallel_hybrid_ais_pso():\n",
    "\n",
    "    search_space = {\n",
    "        \"filters\": [16, 32, 64],\n",
    "        \"kernel_size\": [2, 3, 5],\n",
    "        \"lstm_units\": [16, 32, 64],\n",
    "        \"dropout\": [0.1, 0.2, 0.3],\n",
    "        \"lr\": [0.001, 0.0005]\n",
    "    }\n",
    "\n",
    "    def random_hp():\n",
    "        return {\n",
    "            \"filters\": random.choice(search_space[\"filters\"]),\n",
    "            \"kernel_size\": random.choice(search_space[\"kernel_size\"]),\n",
    "            \"lstm_units\": random.choice(search_space[\"lstm_units\"]),\n",
    "            \"dropout\": random.choice(search_space[\"dropout\"]),\n",
    "            \"lr\": random.choice(search_space[\"lr\"])\n",
    "        }\n",
    "\n",
    "    def fitness(hp):\n",
    "        model = build_model(hp)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=4,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=0\n",
    "        )\n",
    "        return history.history[\"val_loss\"][-1]\n",
    "\n",
    "    # AIS & PSO populations running independently\n",
    "    ais_pop = [random_hp() for _ in range(6)]\n",
    "    pso_pop = [random_hp() for _ in range(6)]\n",
    "    \n",
    "    velocities = [{} for _ in pso_pop]\n",
    "\n",
    "    best_hp = None\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for gen in range(5):\n",
    "        print(f\"[PARALLEL HYBRID] Generation {gen+1}/5\")\n",
    "\n",
    "        # =============== AIS ===============\n",
    "        ais_scores = []\n",
    "        for hp in ais_pop:\n",
    "            s = fitness(hp)\n",
    "            ais_scores.append(s)\n",
    "\n",
    "            if s < best_loss:\n",
    "                best_loss = s\n",
    "                best_hp = hp\n",
    "\n",
    "        # Clone top 50% (CLONALG)\n",
    "        top_idx = np.argsort(ais_scores)[:3]\n",
    "        clones = []\n",
    "        for idx in top_idx:\n",
    "            parent = ais_pop[idx]\n",
    "            for _ in range(2):\n",
    "                mutated = parent.copy()\n",
    "                key = random.choice(list(mutated.keys()))\n",
    "                mutated[key] = random.choice(search_space[key])\n",
    "                clones.append(mutated)\n",
    "\n",
    "        ais_pop = ais_pop + clones\n",
    "        ais_pop = ais_pop[:6]\n",
    "\n",
    "        # =============== PSO ===============\n",
    "        pso_scores = []\n",
    "        for i, hp in enumerate(pso_pop):\n",
    "            s = fitness(hp)\n",
    "            pso_scores.append(s)\n",
    "\n",
    "            if s < best_loss:\n",
    "                best_loss = s\n",
    "                best_hp = hp\n",
    "\n",
    "            for key in hp:\n",
    "                if random.random() < 0.2:\n",
    "                    hp[key] = best_hp[key]\n",
    "\n",
    "    return best_hp\n",
    "\n",
    "# ==============================================================\n",
    "# RUN PARALLEL HYBRID OPTIMIZER\n",
    "# ==============================================================\n",
    "\n",
    "best_hp = parallel_hybrid_ais_pso()\n",
    "print(\"\\n[INFO] BEST HYPERPARAMETERS:\", best_hp)\n",
    "\n",
    "# ==============================================================\n",
    "# TRAIN FINAL MODEL\n",
    "# ==============================================================\n",
    "\n",
    "final_model = build_model(best_hp)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==============================================================\n",
    "# EVALUATE MODEL\n",
    "# ==============================================================\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"[RESULT] MAE={mae:.4f} | RMSE={rmse:.4f}\")\n",
    "\n",
    "# ==============================================================\n",
    "# SAVE FILES\n",
    "# ==============================================================\n",
    "\n",
    "final_model.save(os.path.join(BASE_DIR, \"parallel_quakeguard_model.h5\"))\n",
    "pickle.dump(scaler, open(os.path.join(BASE_DIR, \"parallel_quakeguard_scaler.pkl\"), \"wb\"))\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"parallel_quakeguard_config.yaml\"), \"w\") as f:\n",
    "    yaml.dump({\"best_hyperparameters\": best_hp}, f)\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"parallel_quakeguard_predictions.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"actual\": y_test.flatten().tolist(),\n",
    "        \"predicted\": y_pred.flatten().tolist(),\n",
    "        \"mae\": float(mae),\n",
    "        \"rmse\": float(rmse)\n",
    "    }, f)\n",
    "\n",
    "# ==============================================================\n",
    "# VISUAL GRAPHS\n",
    "# ==============================================================\n",
    "\n",
    "# HEATMAP\n",
    "plt.figure(figsize=(6,5))\n",
    "corr = pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred.flatten()}).corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"parallel_quakeguard_heatmap.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ACCURACY (MAE)\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"mae\"])\n",
    "plt.plot(history.history[\"val_mae\"])\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"parallel_quakeguard_accuracy.png\"))\n",
    "plt.close()\n",
    "\n",
    "# LOSS\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"parallel_quakeguard_loss.png\"))\n",
    "plt.close()\n",
    "\n",
    "# COMPARISON\n",
    "plt.figure()\n",
    "plt.plot(y_test[:200])\n",
    "plt.plot(y_pred[:200])\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"parallel_quakeguard_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "# PREDICTION SCATTER\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"parallel_quakeguard_prediction.png\"))\n",
    "plt.close()\n",
    "\n",
    "# DISTRIBUTION\n",
    "plt.figure()\n",
    "plt.hist(y_pred, bins=30)\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"parallel_quakeguard_result_distribution.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n[INFO] All parallel_ files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21267f2-f5c9-4dd7-9da9-28490d5ded4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
