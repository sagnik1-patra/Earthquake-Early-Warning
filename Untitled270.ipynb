{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df325c01-a429-41ab-91cf-4451393564aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_41880\\3957489467.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_41880\\3957489467.py:40: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_41880\\3957489467.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_41880\\3957489467.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_41880\\3957489467.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_41880\\3957489467.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_41880\\3957489467.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_41880\\3957489467.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_41880\\3957489467.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADAPTIVE HYBRID] Generation 1/6\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "[ADAPTIVE HYBRID] Generation 2/6\n",
      "[ADAPTIVE HYBRID] Generation 3/6\n",
      "[ADAPTIVE HYBRID] Generation 4/6\n",
      "[ADAPTIVE HYBRID] Generation 5/6\n",
      "[ADAPTIVE HYBRID] Generation 6/6\n",
      "\n",
      "[INFO] BEST HYPERPARAMETERS: {'filters': 64, 'kernel_size': 5, 'lstm_units': 32, 'dropout': 0.2, 'lr': 0.001}\n",
      "Epoch 1/25\n",
      "469/469 [==============================] - 6s 7ms/step - loss: 22103713150622760960.0000 - mae: 2396482304.0000 - val_loss: 21687754707695566848.0000 - val_mae: 2351396608.0000\n",
      "Epoch 2/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22103596602390216704.0000 - mae: 2396509440.0000 - val_loss: 21687589780951400448.0000 - val_mae: 2351418624.0000\n",
      "Epoch 3/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22103328321553039360.0000 - mae: 2396534272.0000 - val_loss: 21687189558718889984.0000 - val_mae: 2351475968.0000\n",
      "Epoch 4/25\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 22102736784297295872.0000 - mae: 2396612352.0000 - val_loss: 21686472677137580032.0000 - val_mae: 2351575296.0000\n",
      "Epoch 5/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22101879165227630592.0000 - mae: 2396732672.0000 - val_loss: 21685425942067937280.0000 - val_mae: 2351719680.0000\n",
      "Epoch 6/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22100616925878943744.0000 - mae: 2396900096.0000 - val_loss: 21684027363277406208.0000 - val_mae: 2351915264.0000\n",
      "Epoch 7/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22098980852576813056.0000 - mae: 2397117952.0000 - val_loss: 21682235159324131328.0000 - val_mae: 2352166656.0000\n",
      "Epoch 8/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22096931362902638592.0000 - mae: 2397390592.0000 - val_loss: 21680044932161601536.0000 - val_mae: 2352473856.0000\n",
      "Epoch 9/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22094444267600609280.0000 - mae: 2397726720.0000 - val_loss: 21677419298394472448.0000 - val_mae: 2352839936.0000\n",
      "Epoch 10/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22091530561787002880.0000 - mae: 2398108160.0000 - val_loss: 21674338466813444096.0000 - val_mae: 2353270784.0000\n",
      "Epoch 11/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22088216633740886016.0000 - mae: 2398563840.0000 - val_loss: 21670833223744094208.0000 - val_mae: 2353762048.0000\n",
      "Epoch 12/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22084337556718092288.0000 - mae: 2399084288.0000 - val_loss: 21666826603372478464.0000 - val_mae: 2354322176.0000\n",
      "Epoch 13/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22079961500439543808.0000 - mae: 2399652608.0000 - val_loss: 21662331799838130176.0000 - val_mae: 2354953216.0000\n",
      "Epoch 14/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22075145639509884928.0000 - mae: 2400326656.0000 - val_loss: 21657322424861982720.0000 - val_mae: 2355655424.0000\n",
      "Epoch 15/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22069667872580304896.0000 - mae: 2401045760.0000 - val_loss: 21651754497978925056.0000 - val_mae: 2356435456.0000\n",
      "Epoch 16/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22063745902953103360.0000 - mae: 2401841152.0000 - val_loss: 21645665402584301568.0000 - val_mae: 2357289216.0000\n",
      "Epoch 17/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22057263182395736064.0000 - mae: 2402728704.0000 - val_loss: 21639004561143234560.0000 - val_mae: 2358223616.0000\n",
      "Epoch 18/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22050177929466347520.0000 - mae: 2403676928.0000 - val_loss: 21631758779516190720.0000 - val_mae: 2359239680.0000\n",
      "Epoch 19/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22042103116071960576.0000 - mae: 2404701184.0000 - val_loss: 21623848892865970176.0000 - val_mae: 2360350208.0000\n",
      "Epoch 20/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22033746827700862976.0000 - mae: 2405832704.0000 - val_loss: 21615325478727450624.0000 - val_mae: 2361549056.0000\n",
      "Epoch 21/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22024847380585644032.0000 - mae: 2407073536.0000 - val_loss: 21606179741007609856.0000 - val_mae: 2362833920.0000\n",
      "Epoch 22/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22015013348586815488.0000 - mae: 2408372736.0000 - val_loss: 21596374296311103488.0000 - val_mae: 2364214528.0000\n",
      "Epoch 23/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 22004996797657776128.0000 - mae: 2409817088.0000 - val_loss: 21585924537800720384.0000 - val_mae: 2365684992.0000\n",
      "Epoch 24/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 21993513498217283584.0000 - mae: 2411240704.0000 - val_loss: 21574731509429960704.0000 - val_mae: 2367262208.0000\n",
      "Epoch 25/25\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 21981854276916346880.0000 - mae: 2412873728.0000 - val_loss: 21562786415105802240.0000 - val_mae: 2368943360.0000\n",
      "147/147 [==============================] - 1s 2ms/step\n",
      "[RESULT] MAE=2404361728.0000 | RMSE=4678636835.0986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] All adaptive_ files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Dropout, Flatten, LSTM, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# PATHS\n",
    "# ==============================================================\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\NXTWAVE\\Downloads\\Earthquake Early Warning\\archive\\database.csv\"\n",
    "BASE_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Earthquake Early Warning\"\n",
    "VISUAL_DIR = os.path.join(BASE_DIR, \"visuals\")\n",
    "os.makedirs(VISUAL_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# LOAD + CLEAN DATA\n",
    "# ==============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Convert dates â†’ numeric timestamps\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[col] = df[col].astype(np.int64) // 10**9\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Only numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "df = df.dropna()\n",
    "\n",
    "# Features & target\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SCALING + RESHAPE FOR CNN/LSTM\n",
    "# ==============================================================\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ==============================================================\n",
    "\n",
    "def build_model(hp):\n",
    "    inp = Input(shape=(X_train.shape[1], 1))\n",
    "\n",
    "    x = Conv1D(hp[\"filters\"], hp[\"kernel_size\"], activation=\"relu\")(inp)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    y = LSTM(hp[\"lstm_units\"], return_sequences=False)(inp)\n",
    "\n",
    "    c = Concatenate()([x, y])\n",
    "    c = Dense(64, activation=\"relu\")(c)\n",
    "    c = Dropout(hp[\"dropout\"])(c)\n",
    "    out = Dense(1)(c)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=Adam(hp[\"lr\"]), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# ADAPTIVE HYBRID AIS + PSO OPTIMIZER\n",
    "# ==============================================================\n",
    "\n",
    "def adaptive_hybrid_ais_pso():\n",
    "\n",
    "    search_space = {\n",
    "        \"filters\": [16, 32, 64],\n",
    "        \"kernel_size\": [2, 3, 5],\n",
    "        \"lstm_units\": [16, 32, 64],\n",
    "        \"dropout\": [0.1, 0.2, 0.3],\n",
    "        \"lr\": [0.001, 0.0005]\n",
    "    }\n",
    "\n",
    "    def random_hp():\n",
    "        return {\n",
    "            \"filters\": random.choice(search_space[\"filters\"]),\n",
    "            \"kernel_size\": random.choice(search_space[\"kernel_size\"]),\n",
    "            \"lstm_units\": random.choice(search_space[\"lstm_units\"]),\n",
    "            \"dropout\": random.choice(search_space[\"dropout\"]),\n",
    "            \"lr\": random.choice(search_space[\"lr\"])\n",
    "        }\n",
    "\n",
    "    def fitness(hp):\n",
    "        model = build_model(hp)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=4,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=0\n",
    "        )\n",
    "        return history.history[\"val_loss\"][-1]\n",
    "\n",
    "    pop = [random_hp() for _ in range(8)]\n",
    "    velocities = [{} for _ in pop]\n",
    "\n",
    "    best_hp = None\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    ais_weight = 0.5\n",
    "    pso_weight = 0.5\n",
    "\n",
    "    for gen in range(6):\n",
    "        print(f\"[ADAPTIVE HYBRID] Generation {gen+1}/6\")\n",
    "\n",
    "        scores = []\n",
    "        for hp in pop:\n",
    "            s = fitness(hp)\n",
    "            scores.append(s)\n",
    "\n",
    "            if s < best_loss:\n",
    "                best_loss = s\n",
    "                best_hp = hp\n",
    "\n",
    "        sorted_idx = np.argsort(scores)\n",
    "        best_list = [pop[i] for i in sorted_idx[:4]]\n",
    "\n",
    "        # AIS behaviour (mutation)\n",
    "        ais_children = []\n",
    "        for parent in best_list:\n",
    "            for _ in range(2):\n",
    "                child = parent.copy()\n",
    "                key = random.choice(list(child.keys()))\n",
    "                child[key] = random.choice(search_space[key])\n",
    "                ais_children.append(child)\n",
    "\n",
    "        # PSO behaviour\n",
    "        pso_children = []\n",
    "        for hp in pop:\n",
    "            new_hp = hp.copy()\n",
    "            for key in new_hp:\n",
    "                if random.random() < pso_weight:\n",
    "                    new_hp[key] = best_hp[key]\n",
    "            pso_children.append(new_hp)\n",
    "\n",
    "        # ADAPTIVE CONTROL\n",
    "        if scores[sorted_idx[0]] < best_loss * 0.98:\n",
    "            ais_weight += 0.1\n",
    "        else:\n",
    "            pso_weight += 0.1\n",
    "\n",
    "        ais_weight = min(0.9, ais_weight)\n",
    "        pso_weight = min(0.9, pso_weight)\n",
    "\n",
    "        pop = ais_children[:4] + pso_children[:4]\n",
    "\n",
    "    return best_hp\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# RUN ADAPTIVE OPTIMIZER\n",
    "# ==============================================================\n",
    "\n",
    "best_hp = adaptive_hybrid_ais_pso()\n",
    "print(\"\\n[INFO] BEST HYPERPARAMETERS:\", best_hp)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# TRAIN FINAL MODEL\n",
    "# ==============================================================\n",
    "\n",
    "final_model = build_model(best_hp)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# EVALUATE MODEL\n",
    "# ==============================================================\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"[RESULT] MAE={mae:.4f} | RMSE={rmse:.4f}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SAVE FILES\n",
    "# ==============================================================\n",
    "\n",
    "final_model.save(os.path.join(BASE_DIR, \"adaptive_quakeguard_model.h5\"))\n",
    "pickle.dump(scaler, open(os.path.join(BASE_DIR, \"adaptive_quakeguard_scaler.pkl\"), \"wb\"))\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"adaptive_quakeguard_config.yaml\"), \"w\") as f:\n",
    "    yaml.dump({\"best_hyperparameters\": best_hp}, f)\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"adaptive_quakeguard_predictions.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"actual\": y_test.flatten().tolist(),\n",
    "        \"predicted\": y_pred.flatten().tolist(),\n",
    "        \"mae\": float(mae),\n",
    "        \"rmse\": float(rmse)\n",
    "    }, f)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# VISUAL GRAPHS\n",
    "# ==============================================================\n",
    "\n",
    "# HEATMAP\n",
    "plt.figure(figsize=(6,5))\n",
    "corr = pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred.flatten()}).corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"adaptive_quakeguard_heatmap.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ACCURACY\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"mae\"])\n",
    "plt.plot(history.history[\"val_mae\"])\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"adaptive_quakeguard_accuracy.png\"))\n",
    "plt.close()\n",
    "\n",
    "# LOSS\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"adaptive_quakeguard_loss.png\"))\n",
    "plt.close()\n",
    "\n",
    "# COMPARISON\n",
    "plt.figure()\n",
    "plt.plot(y_test[:200])\n",
    "plt.plot(y_pred[:200])\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"adaptive_quakeguard_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "# SCATTER\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"adaptive_quakeguard_prediction.png\"))\n",
    "plt.close()\n",
    "\n",
    "# DISTRIBUTION\n",
    "plt.figure()\n",
    "plt.hist(y_pred, bins=30)\n",
    "plt.savefig(os.path.join(VISUAL_DIR, \"adaptive_quakeguard_result_distribution.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n[INFO] All adaptive_ files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071fa1f-e997-4241-8448-c465b83eafcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
